{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1299795,"sourceType":"datasetVersion","datasetId":751906},{"sourceId":2006709,"sourceType":"datasetVersion","datasetId":1200659},{"sourceId":5634325,"sourceType":"datasetVersion","datasetId":3226482}],"dockerImageVersionId":30474,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Part 1 U-Net Model Segmentation**","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-08T08:18:11.060258Z","iopub.execute_input":"2023-05-08T08:18:11.061065Z","iopub.status.idle":"2023-05-08T08:18:11.070766Z","shell.execute_reply.started":"2023-05-08T08:18:11.061025Z","shell.execute_reply":"2023-05-08T08:18:11.069492Z"}}},{"cell_type":"code","source":"import os\nimport cv2\nimport glob\nimport PIL\nimport shutil\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom skimage import data\nfrom skimage.util import montage\nimport skimage.transform as skTrans\nfrom skimage.transform import rotate\nfrom skimage.transform import resize\nfrom PIL import Image, ImageOps\nimport nilearn as nl\nimport nibabel as nib\n\n# ml libs\nimport keras\nimport keras.backend as K\nfrom keras.callbacks import CSVLogger\nimport tensorflow as tf\nfrom tensorflow.keras.utils import plot_model\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TensorBoard\n\n\n\n\nnp.set_printoptions(precision=3, suppress=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-09T07:55:28.124383Z","iopub.execute_input":"2023-05-09T07:55:28.124801Z","iopub.status.idle":"2023-05-09T07:55:28.134656Z","shell.execute_reply.started":"2023-05-09T07:55:28.124764Z","shell.execute_reply":"2023-05-09T07:55:28.133525Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nLabel definition and required parameters\n\"\"\"\nSEGMENT_CLASSES = {\n0 : 'No Tumor',\n1 : 'Necrotic/Nuclear', #Tumor Nucleus\n2 : 'Edema',#Edema Part\n3 : 'Expanding Tumor' #Expanding Tumor Part\n}\n\nIMG_SIZE=64\n\n\nVOLUME_SLICES = 100\nVOLUME_START_AT = 22 ","metadata":{"execution":{"iopub.status.busy":"2023-05-09T07:55:28.136859Z","iopub.execute_input":"2023-05-09T07:55:28.137705Z","iopub.status.idle":"2023-05-09T07:55:28.145402Z","shell.execute_reply.started":"2023-05-09T07:55:28.137666Z","shell.execute_reply":"2023-05-09T07:55:28.144285Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Verilerin ön incelenmesi ve görselleştirilmesi\nTRAIN_DATASET_PATH = '/kaggle/input/brain-tumor-segmentation-brats-2019/MICCAI_BraTS_2019_Data_Training/HGG'\nVALIDATION_DATASET_PATH = '/kaggle/input/brain-tumor-segmentation-brats-2019/MICCAI_BraTS_2019_Data_Training/LGG'\n","metadata":{"execution":{"iopub.status.busy":"2023-05-09T07:55:28.146652Z","iopub.execute_input":"2023-05-09T07:55:28.147043Z","iopub.status.idle":"2023-05-09T07:55:28.157185Z","shell.execute_reply.started":"2023-05-09T07:55:28.147004Z","shell.execute_reply":"2023-05-09T07:55:28.156402Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"Bu kod, böbrek ve beyin tümörlerinin segmentasyonu için Dice katsayısı, doğruluk, duyarlılık ve özgüllük gibi değerlerin hesaplanmasını sağlar.\ndice_coef fonksiyonu, verilen gerçek ve tahmini etiketler arasında Dice katsayısını hesaplar ve döndürür. \ndice_coef_necrotic, dice_coef_edema ve dice_coef_enhancing fonksiyonları ise sırasıyla nekrotik, ödemli ve genişleyen tümör bölgeleri için \nDice katsayısını hesaplar.precision fonksiyonu, verilen gerçek ve tahmini etiketler arasında doğruluk değerini hesaplar ve döndürür.\nsensitivity fonksiyonu, verilen gerçek etiketler ve tahmini etiketler arasında duyarlılık değerini hesaplar ve döndürür.\nspecificity fonksiyonu ise verilen gerçek etiketler ve tahmini etiketler arasında özgüllük değerini hesaplar ve döndürür.\"\"\"\n\ndef dice_coef(y_true, y_pred, smooth=1.0):\n    class_num = 4\n    for i in range(class_num):\n        y_true_f = K.flatten(y_true[:,:,:,i])\n        y_pred_f = K.flatten(y_pred[:,:,:,i])\n        intersection = K.sum(y_true_f * y_pred_f)\n        loss = ((2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth))\n  \n        if i == 0:\n            total_loss = loss\n        else:\n            total_loss = total_loss + loss\n    total_loss = total_loss / class_num\n    return total_loss\n\n\ndef dice_coef_necrotic(y_true, y_pred, epsilon=1e-6):\n    intersection = K.sum(K.abs(y_true[:,:,:,1] * y_pred[:,:,:,1]))\n    return (2. * intersection) / (K.sum(K.square(y_true[:,:,:,1])) + K.sum(K.square(y_pred[:,:,:,1])) + epsilon)\n\ndef dice_coef_edema(y_true, y_pred, epsilon=1e-6):\n    intersection = K.sum(K.abs(y_true[:,:,:,2] * y_pred[:,:,:,2]))\n    return (2. * intersection) / (K.sum(K.square(y_true[:,:,:,2])) + K.sum(K.square(y_pred[:,:,:,2])) + epsilon)\n\ndef dice_coef_enhancing(y_true, y_pred, epsilon=1e-6):\n    intersection = K.sum(K.abs(y_true[:,:,:,3] * y_pred[:,:,:,3]))\n    return (2. * intersection) / (K.sum(K.square(y_true[:,:,:,3])) + K.sum(K.square(y_pred[:,:,:,3])) + epsilon)\n\n\n\ndef precision(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives / (predicted_positives + K.epsilon())\n        return precision\n\n\ndef sensitivity(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    return true_positives / (possible_positives + K.epsilon())\n\n\ndef specificity(y_true, y_pred):\n    true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n    possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))\n    return true_negatives / (possible_negatives + K.epsilon())","metadata":{"execution":{"iopub.status.busy":"2023-05-09T07:55:28.159209Z","iopub.execute_input":"2023-05-09T07:55:28.159731Z","iopub.status.idle":"2023-05-09T07:55:28.174497Z","shell.execute_reply.started":"2023-05-09T07:55:28.159696Z","shell.execute_reply":"2023-05-09T07:55:28.173681Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.listdir(TRAIN_DATASET_PATH)\nos.listdir(VALIDATION_DATASET_PATH)","metadata":{"execution":{"iopub.status.busy":"2023-05-09T07:55:28.175803Z","iopub.execute_input":"2023-05-09T07:55:28.176741Z","iopub.status.idle":"2023-05-09T07:55:28.232786Z","shell.execute_reply.started":"2023-05-09T07:55:28.176692Z","shell.execute_reply":"2023-05-09T07:55:28.231680Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(os.listdir(TRAIN_DATASET_PATH)))\nprint(len(os.listdir(VALIDATION_DATASET_PATH)))","metadata":{"execution":{"iopub.status.busy":"2023-05-09T07:55:28.235731Z","iopub.execute_input":"2023-05-09T07:55:28.236112Z","iopub.status.idle":"2023-05-09T07:55:28.242688Z","shell.execute_reply.started":"2023-05-09T07:55:28.236082Z","shell.execute_reply":"2023-05-09T07:55:28.241641Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Modelin yüklenmesi\n#Modelin tahmin aşamasında kullanması için eğitim sonrası değerleri tekrar yükledik.\nmodel = keras.models.load_model('/kaggle/input/bratsparts/u-net_part1.h5', \n                                   custom_objects={ 'accuracy' : tf.keras.metrics.MeanIoU(num_classes=4),\n                                                   \"dice_coef\": dice_coef,\n                                                   \"precision\": precision,\n                                                   \"sensitivity\":sensitivity,\n                                                   \"specificity\":specificity,\n                                                   \"dice_coef_necrotic\": dice_coef_necrotic,\n                                                   \"dice_coef_edema\": dice_coef_edema,\n                                                   \"dice_coef_enhancing\": dice_coef_enhancing\n                                                  }, compile=False)\n\n#Eğitim sonrası bize grafikleri verdi \nhistory = pd.read_csv('/kaggle/input/bratsparts/egitim_part1.log', sep=',', engine='python')\n\nhist=history\n\n\n\nacc=hist['accuracy']\nval_acc=hist['val_accuracy']\n\nepoch=range(len(acc))\n\nloss=hist['loss']\nval_loss=hist['val_loss']\n\ntrain_dice=hist['dice_coef']\nval_dice=hist['val_dice_coef']\n\nf,ax=plt.subplots(1,3,figsize=(16,8))\n\nax[0].plot(epoch,acc,'b',label='Training Accuracy')\nax[0].plot(epoch,val_acc,'r',label='Validation Accuracy')\nax[0].legend()\n\nax[1].plot(epoch,loss,'b',label='Training Loss')\nax[1].plot(epoch,val_loss,'r',label='Validation Loss')\nax[1].legend()\n\nax[2].plot(epoch,train_dice,'b',label='Training dice coef')\nax[2].plot(epoch,val_dice,'r',label='Validation dice coef')\nax[2].legend()\n\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-09T07:55:28.246642Z","iopub.execute_input":"2023-05-09T07:55:28.247778Z","iopub.status.idle":"2023-05-09T07:55:30.354015Z","shell.execute_reply.started":"2023-05-09T07:55:28.247731Z","shell.execute_reply":"2023-05-09T07:55:30.352868Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the model\nmodel = keras.models.load_model('/kaggle/input/bratsparts/u-net_part1.h5', \n                                custom_objects={ 'accuracy' : tf.keras.metrics.MeanIoU(num_classes=4),\n                                                \"dice_coef\": dice_coef,\n                                                \"precision\": precision,\n                                                \"sensitivity\":sensitivity,\n                                                \"specificity\":specificity,\n                                                \"dice_coef_necrotic\": dice_coef_necrotic,\n                                                \"dice_coef_edema\": dice_coef_edema,\n                                                \"dice_coef_enhancing\": dice_coef_enhancing\n                                               }, compile=False)\n\n# Load the training history\nhistory = pd.read_csv('/kaggle/input/bratsparts/egitim_part1.log', sep=',', engine='python')\n\n# Plot the performance metrics\nf, ax = plt.subplots(2, 2, figsize=(16, 12))\n\nax[0, 0].plot(history['accuracy'], label='Training accuracy')\nax[0, 0].plot(history['val_accuracy'], label='Validation accuracy')\nax[0, 0].set_xlabel('Epoch')\nax[0, 0].set_ylabel('Accuracy')\nax[0, 0].legend()\n\nax[0, 1].plot(history['loss'], label='Training loss')\nax[0, 1].plot(history['val_loss'], label='Validation loss')\nax[0, 1].set_xlabel('Epoch')\nax[0, 1].set_ylabel('Loss')\nax[0, 1].legend()\n\nax[1, 0].plot(history['dice_coef'], label='Training dice coefficient')\nax[1, 0].plot(history['val_dice_coef'], label='Validation dice coefficient')\nax[1, 0].set_xlabel('Epoch')\nax[1, 0].set_ylabel('Dice coefficient')\nax[1, 0].legend()\n\nax[1, 1].plot(history['accuracy'], label='Training accuracy')\nax[1, 1].plot(history['val_accuracy'], label='Validation accuracy')\nax[1, 1].plot(history['dice_coef'], label='Training dice coefficient')\nax[1, 1].plot(history['val_dice_coef'], label='Validation dice coefficient')\nax[1, 1].set_xlabel('Epoch')\nax[1, 1].set_ylabel('Metric')\nax[1, 1].legend()\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-05-09T07:55:30.356475Z","iopub.execute_input":"2023-05-09T07:55:30.357179Z","iopub.status.idle":"2023-05-09T07:55:31.672218Z","shell.execute_reply.started":"2023-05-09T07:55:30.357135Z","shell.execute_reply":"2023-05-09T07:55:31.671398Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\" verilen mr tipleri bunlardan birisi olmalıdır         1) flair -- 2) t1-- 3) t1ce -- 4) -- t2  5) seg \nGirdi olarak verilen dosya tahmin edilebilir hale çeviriliyor\"\"\"\ndef imageLoader(path):\n    image = nib.load(path).get_fdata()\n    X = np.zeros((self.batch_sizeVOLUME_SLICES,self.dim, self.n_channels))\n    for j in range(VOLUME_SLICES):\n        X[j +VOLUME_SLICESc,:,:,0] = cv2.resize(image[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE));\n        X[j +VOLUME_SLICESc,:,:,1] = cv2.resize(ce[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE));\n\n        y[j +VOLUME_SLICESc] = seg[:,:,j+VOLUME_START_AT];\n    return np.array(image)\n\n\"\"\"Bu fonksiyon, bir dizin yoludur, veri dosyalarının bulunduğu alt dizinlerin listesi ve yüklenen MRI görüntülerinin türü gibi argümanlar alır. \nDaha sonra, veri ve maske dosyalarını okur, yeniden boyutlandırır ve scans ve masks listelerine ekler. \nBu listeler daha sonra numpy dizileri olarak döndürülür.Bunu da veriyi dizinden yüklemek için kullanıyoruz. \"\"\"\n\ndef loadDataFromDir(path, list_of_files, mriType, n_images):\n    scans = []\n    masks = []\n    for i in list_of_files[:n_images]:\n        fullPath = glob.glob( i + '/'+ mriType +'')[0]\n        currentScanVolume = imageLoader(fullPath)\n        currentMaskVolume = imageLoader( glob.glob( i + '/seg*')[0] ) \n        \n        for j in range(0, currentScanVolume.shape[2]):\n            scan_img = cv2.resize(currentScanVolume[:,:,j], dsize=(IMG_SIZE,IMG_SIZE), interpolation=cv2.INTER_AREA).astype('uint8')\n            mask_img = cv2.resize(currentMaskVolume[:,:,j], dsize=(IMG_SIZE,IMG_SIZE), interpolation=cv2.INTER_AREA).astype('uint8')\n            scans.append(scan_img[..., np.newaxis])\n            masks.append(mask_img[..., np.newaxis])\n    return np.array(scans, dtype='float32'), np.array(masks, dtype='float32')\n\n","metadata":{"execution":{"iopub.status.busy":"2023-05-09T07:55:31.673625Z","iopub.execute_input":"2023-05-09T07:55:31.673996Z","iopub.status.idle":"2023-05-09T07:55:31.685427Z","shell.execute_reply.started":"2023-05-09T07:55:31.673963Z","shell.execute_reply":"2023-05-09T07:55:31.684346Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"Bu kod, eğitilmiş bir modeli kullanarak bir dizinde saklanan MRI taramaları üzerinde tahminler yapmaya yönelik görünüyor.\nÖnce nib.load fonksiyonunu kullanarak tarama verilerini yükler.\nDaha sonra, cv2.resize kullanılarak taramalar yeniden boyutlandırılır ve X olarak adlandırılan 3D bir diziye kaydedilir. \npredictByPath fonksiyonu daha sonra modeli kullanarak X üzerinde tahminler yapar ve tahminleri bir dizi olarak döndürür.\nshowPredictsById fonksiyonu, modelin yaptığı tahminleri görselleştirmeyi amaçlı görünüyor. Bu fonksiyon, taramalar için ground truth verilerini yükler\"\"\"\n\n\ndef predictByPath(case_path,case):\n    files = next(os.walk(case_path))[2]\n    X = np.empty((VOLUME_SLICES, IMG_SIZE, IMG_SIZE, 2))\n\n    vol_path = os.path.join(case_path, f'BraTS19_2013_{case}_flair.nii');\n    flair=nib.load(vol_path).get_fdata()\n\n    vol_path = os.path.join(case_path, f'BraTS19_2013_{case}_t1ce.nii');\n    ce=nib.load(vol_path).get_fdata() \n\n    for j in range(VOLUME_SLICES):\n        X[j,:,:,0] = cv2.resize(flair[:,:,j+VOLUME_START_AT], (IMG_SIZE,IMG_SIZE))\n        X[j,:,:,1] = cv2.resize(ce[:,:,j+VOLUME_START_AT], (IMG_SIZE,IMG_SIZE))\n  \n    return model.predict(X/np.max(X), verbose=1)\n\ndef showPredictsById(case, start_slice = 60):\n    path = f\"/kaggle/input/brain-tumor-segmentation-brats-2019/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_2013_{case}\"\n    gt = nib.load(os.path.join(path, f'BraTS19_2013_{case}_seg.nii')).get_fdata()\n    origImage = nib.load(os.path.join(path, f'BraTS19_2013_{case}_flair.nii')).get_fdata()\n    p = predictByPath(path,case)\n\n    core = p[:,:,:,1]\n    edema= p[:,:,:,2]\n    enhancing = p[:,:,:,3]\n\n    plt.figure(figsize=(18, 50))\n    f, axarr = plt.subplots(1,6, figsize = (18, 50)) \n\n    for i in range(6):\n        axarr[i].imshow(cv2.resize(origImage[:,:,start_slice+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE)),cmap=\"gray\", interpolation='none')\n\n    axarr[0].imshow(cv2.resize(origImage[:,:,start_slice+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE)), cmap=\"gray\")\n    axarr[0].title.set_text('Original image flair')\n    curr_gt=cv2.resize(gt[:,:,start_slice+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE), interpolation = cv2.INTER_NEAREST)\n    axarr[1].imshow(curr_gt, cmap=\"Reds\", interpolation='none', alpha=0.3) # ,alpha=0.3,cmap='Reds'\n    axarr[1].title.set_text('Ground truth')\n    axarr[2].imshow(p[start_slice,:,:,1:4], cmap=\"Reds\", interpolation='none', alpha=0.3)\n    axarr[2].title.set_text('all classes')\n    axarr[3].imshow(edema[start_slice,:,:], cmap=\"OrRd\", interpolation='none', alpha=0.3)\n    axarr[3].title.set_text(f'{SEGMENT_CLASSES[1]} predicted')\n    axarr[4].imshow(core[start_slice,:,], cmap=\"OrRd\", interpolation='none', alpha=0.3)\n    axarr[4].title.set_text(f'{SEGMENT_CLASSES[2]} predicted')\n    axarr[5].imshow(enhancing[start_slice,:,], cmap=\"OrRd\", interpolation='none', alpha=0.3)\n    axarr[5].title.set_text(f'{SEGMENT_CLASSES[3]} predicted')\n    plt.show()\n\n\nshowPredictsById(case=\"5_1\")#veri deneme\n","metadata":{"execution":{"iopub.status.busy":"2023-05-09T07:55:31.686880Z","iopub.execute_input":"2023-05-09T07:55:31.687235Z","iopub.status.idle":"2023-05-09T07:55:36.542589Z","shell.execute_reply.started":"2023-05-09T07:55:31.687206Z","shell.execute_reply":"2023-05-09T07:55:36.541370Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Part 2 U-Net Model Segmentation**","metadata":{}},{"cell_type":"code","source":"#Modelin yüklenmesi\n#Modelin tahmin aşamasında kullanması için eğitim sonrası değerleri tekrar yükledik.\nmodel = keras.models.load_model('/kaggle/input/bratsparts/u-net_part2.h5', \n                                   custom_objects={ 'accuracy' : tf.keras.metrics.MeanIoU(num_classes=4),\n                                                   \"dice_coef\": dice_coef,\n                                                   \"precision\": precision,\n                                                   \"sensitivity\":sensitivity,\n                                                   \"specificity\":specificity,\n                                                   \"dice_coef_necrotic\": dice_coef_necrotic,\n                                                   \"dice_coef_edema\": dice_coef_edema,\n                                                   \"dice_coef_enhancing\": dice_coef_enhancing\n                                                  }, compile=False)\n\n#Eğitim sonrası bize grafikleri verdi \nhistory = pd.read_csv('/kaggle/input/bratsparts/egitim_part2.log', sep=',', engine='python')\n\nhist=history\n\n\n\nacc=hist['accuracy']\nval_acc=hist['val_accuracy']\n\nepoch=range(len(acc))\n\nloss=hist['loss']\nval_loss=hist['val_loss']\n\ntrain_dice=hist['dice_coef']\nval_dice=hist['val_dice_coef']\n\nf,ax=plt.subplots(1,3,figsize=(16,8))\n\nax[0].plot(epoch,acc,'b',label='Training Accuracy')\nax[0].plot(epoch,val_acc,'r',label='Validation Accuracy')\nax[0].legend()\n\nax[1].plot(epoch,loss,'b',label='Training Loss')\nax[1].plot(epoch,val_loss,'r',label='Validation Loss')\nax[1].legend()\n\nax[2].plot(epoch,train_dice,'b',label='Training dice coef')\nax[2].plot(epoch,val_dice,'r',label='Validation dice coef')\nax[2].legend()\n\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-09T07:55:36.545553Z","iopub.execute_input":"2023-05-09T07:55:36.545955Z","iopub.status.idle":"2023-05-09T07:55:38.095432Z","shell.execute_reply.started":"2023-05-09T07:55:36.545912Z","shell.execute_reply":"2023-05-09T07:55:38.094531Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the model\nmodel = keras.models.load_model('/kaggle/input/bratsparts/u-net_part2.h5', \n                                custom_objects={ 'accuracy' : tf.keras.metrics.MeanIoU(num_classes=4),\n                                                \"dice_coef\": dice_coef,\n                                                \"precision\": precision,\n                                                \"sensitivity\":sensitivity,\n                                                \"specificity\":specificity,\n                                                \"dice_coef_necrotic\": dice_coef_necrotic,\n                                                \"dice_coef_edema\": dice_coef_edema,\n                                                \"dice_coef_enhancing\": dice_coef_enhancing\n                                               }, compile=False)\n\n# Load the training history\nhistory = pd.read_csv('/kaggle/input/bratsparts/egitim_part2.log', sep=',', engine='python')\n\n# Plot the performance metrics\nf, ax = plt.subplots(2, 2, figsize=(16, 12))\n\nax[0, 0].plot(history['accuracy'], label='Training accuracy')\nax[0, 0].plot(history['val_accuracy'], label='Validation accuracy')\nax[0, 0].set_xlabel('Epoch')\nax[0, 0].set_ylabel('Accuracy')\nax[0, 0].legend()\n\nax[0, 1].plot(history['loss'], label='Training loss')\nax[0, 1].plot(history['val_loss'], label='Validation loss')\nax[0, 1].set_xlabel('Epoch')\nax[0, 1].set_ylabel('Loss')\nax[0, 1].legend()\n\nax[1, 0].plot(history['dice_coef'], label='Training dice coefficient')\nax[1, 0].plot(history['val_dice_coef'], label='Validation dice coefficient')\nax[1, 0].set_xlabel('Epoch')\nax[1, 0].set_ylabel('Dice coefficient')\nax[1, 0].legend()\n\nax[1, 1].plot(history['accuracy'], label='Training accuracy')\nax[1, 1].plot(history['val_accuracy'], label='Validation accuracy')\nax[1, 1].plot(history['dice_coef'], label='Training dice coefficient')\nax[1, 1].plot(history['val_dice_coef'], label='Validation dice coefficient')\nax[1, 1].set_xlabel('Epoch')\nax[1, 1].set_ylabel('Metric')\nax[1, 1].legend()\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-05-09T07:55:38.096899Z","iopub.execute_input":"2023-05-09T07:55:38.097529Z","iopub.status.idle":"2023-05-09T07:55:39.437987Z","shell.execute_reply.started":"2023-05-09T07:55:38.097493Z","shell.execute_reply":"2023-05-09T07:55:39.436933Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"Bu kod, eğitilmiş bir modeli kullanarak bir dizinde saklanan MRI taramaları üzerinde tahminler yapmaya yönelik görünüyor.\nÖnce nib.load fonksiyonunu kullanarak tarama verilerini yükler.\nDaha sonra, cv2.resize kullanılarak taramalar yeniden boyutlandırılır ve X olarak adlandırılan 3D bir diziye kaydedilir. \npredictByPath fonksiyonu daha sonra modeli kullanarak X üzerinde tahminler yapar ve tahminleri bir dizi olarak döndürür.\nshowPredictsById fonksiyonu, modelin yaptığı tahminleri görselleştirmeyi amaçlı görünüyor. Bu fonksiyon, taramalar için ground truth verilerini yükler\"\"\"\n\n\ndef predictByPath(case_path,case):\n    files = next(os.walk(case_path))[2]\n    X = np.empty((VOLUME_SLICES, IMG_SIZE, IMG_SIZE, 2))\n\n    vol_path = os.path.join(case_path, f'BraTS19_2013_{case}_flair.nii');\n    flair=nib.load(vol_path).get_fdata()\n\n    vol_path = os.path.join(case_path, f'BraTS19_2013_{case}_t1ce.nii');\n    ce=nib.load(vol_path).get_fdata() \n\n    for j in range(VOLUME_SLICES):\n        X[j,:,:,0] = cv2.resize(flair[:,:,j+VOLUME_START_AT], (IMG_SIZE,IMG_SIZE))\n        X[j,:,:,1] = cv2.resize(ce[:,:,j+VOLUME_START_AT], (IMG_SIZE,IMG_SIZE))\n  \n    return model.predict(X/np.max(X), verbose=1)\n\ndef showPredictsById(case, start_slice = 60):\n    path = f\"/kaggle/input/brain-tumor-segmentation-brats-2019/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_2013_{case}\"\n    gt = nib.load(os.path.join(path, f'BraTS19_2013_{case}_seg.nii')).get_fdata()\n    origImage = nib.load(os.path.join(path, f'BraTS19_2013_{case}_flair.nii')).get_fdata()\n    p = predictByPath(path,case)\n\n    core = p[:,:,:,1]\n    edema= p[:,:,:,2]\n    enhancing = p[:,:,:,3]\n\n    plt.figure(figsize=(18, 50))\n    f, axarr = plt.subplots(1,6, figsize = (18, 50)) \n\n    for i in range(6):\n        axarr[i].imshow(cv2.resize(origImage[:,:,start_slice+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE)),cmap=\"gray\", interpolation='none')\n\n    axarr[0].imshow(cv2.resize(origImage[:,:,start_slice+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE)), cmap=\"gray\")\n    axarr[0].title.set_text('Original image flair')\n    curr_gt=cv2.resize(gt[:,:,start_slice+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE), interpolation = cv2.INTER_NEAREST)\n    axarr[1].imshow(curr_gt, cmap=\"Reds\", interpolation='none', alpha=0.3) # ,alpha=0.3,cmap='Reds'\n    axarr[1].title.set_text('Ground truth')\n    axarr[2].imshow(p[start_slice,:,:,1:4], cmap=\"Reds\", interpolation='none', alpha=0.3)\n    axarr[2].title.set_text('all classes')\n    axarr[3].imshow(edema[start_slice,:,:], cmap=\"OrRd\", interpolation='none', alpha=0.3)\n    axarr[3].title.set_text(f'{SEGMENT_CLASSES[1]} predicted')\n    axarr[4].imshow(core[start_slice,:,], cmap=\"OrRd\", interpolation='none', alpha=0.3)\n    axarr[4].title.set_text(f'{SEGMENT_CLASSES[2]} predicted')\n    axarr[5].imshow(enhancing[start_slice,:,], cmap=\"OrRd\", interpolation='none', alpha=0.3)\n    axarr[5].title.set_text(f'{SEGMENT_CLASSES[3]} predicted')\n    plt.show()\n\n\nshowPredictsById(case=\"5_1\")#veri deneme\n","metadata":{"execution":{"iopub.status.busy":"2023-05-09T07:55:39.439553Z","iopub.execute_input":"2023-05-09T07:55:39.439939Z","iopub.status.idle":"2023-05-09T07:55:43.390015Z","shell.execute_reply.started":"2023-05-09T07:55:39.439905Z","shell.execute_reply":"2023-05-09T07:55:43.388765Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Part 3 U-Net Model Segmentation**","metadata":{}},{"cell_type":"code","source":"#Modelin yüklenmesi\n#Modelin tahmin aşamasında kullanması için eğitim sonrası değerleri tekrar yükledik.\nmodel = keras.models.load_model('/kaggle/input/bratsparts/u-net_part3.h5', \n                                   custom_objects={ 'accuracy' : tf.keras.metrics.MeanIoU(num_classes=4),\n                                                   \"dice_coef\": dice_coef,\n                                                   \"precision\": precision,\n                                                   \"sensitivity\":sensitivity,\n                                                   \"specificity\":specificity,\n                                                   \"dice_coef_necrotic\": dice_coef_necrotic,\n                                                   \"dice_coef_edema\": dice_coef_edema,\n                                                   \"dice_coef_enhancing\": dice_coef_enhancing\n                                                  }, compile=False)\n\n#Eğitim sonrası bize grafikleri verdi \nhistory = pd.read_csv('/kaggle/input/bratsparts/egitim_part3.log', sep=',', engine='python')\n\nhist=history\n\n\n\nacc=hist['accuracy']\nval_acc=hist['val_accuracy']\n\nepoch=range(len(acc))\n\nloss=hist['loss']\nval_loss=hist['val_loss']\n\ntrain_dice=hist['dice_coef']\nval_dice=hist['val_dice_coef']\n\nf,ax=plt.subplots(1,3,figsize=(16,8))\n\nax[0].plot(epoch,acc,'b',label='Training Accuracy')\nax[0].plot(epoch,val_acc,'r',label='Validation Accuracy')\nax[0].legend()\n\nax[1].plot(epoch,loss,'b',label='Training Loss')\nax[1].plot(epoch,val_loss,'r',label='Validation Loss')\nax[1].legend()\n\nax[2].plot(epoch,train_dice,'b',label='Training dice coef')\nax[2].plot(epoch,val_dice,'r',label='Validation dice coef')\nax[2].legend()\n\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-09T07:55:43.391280Z","iopub.execute_input":"2023-05-09T07:55:43.391604Z","iopub.status.idle":"2023-05-09T07:55:44.866517Z","shell.execute_reply.started":"2023-05-09T07:55:43.391576Z","shell.execute_reply":"2023-05-09T07:55:44.865671Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the model\nmodel = keras.models.load_model('/kaggle/input/bratsparts/u-net_part3.h5', \n                                custom_objects={ 'accuracy' : tf.keras.metrics.MeanIoU(num_classes=4),\n                                                \"dice_coef\": dice_coef,\n                                                \"precision\": precision,\n                                                \"sensitivity\":sensitivity,\n                                                \"specificity\":specificity,\n                                                \"dice_coef_necrotic\": dice_coef_necrotic,\n                                                \"dice_coef_edema\": dice_coef_edema,\n                                                \"dice_coef_enhancing\": dice_coef_enhancing\n                                               }, compile=False)\n\n# Load the training history\nhistory = pd.read_csv('/kaggle/input/bratsparts/egitim_part3.log', sep=',', engine='python')\n\n# Plot the performance metrics\nf, ax = plt.subplots(2, 2, figsize=(16, 12))\n\nax[0, 0].plot(history['accuracy'], label='Training accuracy')\nax[0, 0].plot(history['val_accuracy'], label='Validation accuracy')\nax[0, 0].set_xlabel('Epoch')\nax[0, 0].set_ylabel('Accuracy')\nax[0, 0].legend()\n\nax[0, 1].plot(history['loss'], label='Training loss')\nax[0, 1].plot(history['val_loss'], label='Validation loss')\nax[0, 1].set_xlabel('Epoch')\nax[0, 1].set_ylabel('Loss')\nax[0, 1].legend()\n\nax[1, 0].plot(history['dice_coef'], label='Training dice coefficient')\nax[1, 0].plot(history['val_dice_coef'], label='Validation dice coefficient')\nax[1, 0].set_xlabel('Epoch')\nax[1, 0].set_ylabel('Dice coefficient')\nax[1, 0].legend()\n\nax[1, 1].plot(history['accuracy'], label='Training accuracy')\nax[1, 1].plot(history['val_accuracy'], label='Validation accuracy')\nax[1, 1].plot(history['dice_coef'], label='Training dice coefficient')\nax[1, 1].plot(history['val_dice_coef'], label='Validation dice coefficient')\nax[1, 1].set_xlabel('Epoch')\nax[1, 1].set_ylabel('Metric')\nax[1, 1].legend()\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-05-09T07:55:44.867900Z","iopub.execute_input":"2023-05-09T07:55:44.868910Z","iopub.status.idle":"2023-05-09T07:55:46.589866Z","shell.execute_reply.started":"2023-05-09T07:55:44.868871Z","shell.execute_reply":"2023-05-09T07:55:46.588570Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"Bu kod, eğitilmiş bir modeli kullanarak bir dizinde saklanan MRI taramaları üzerinde tahminler yapmaya yönelik görünüyor.\nÖnce nib.load fonksiyonunu kullanarak tarama verilerini yükler.\nDaha sonra, cv2.resize kullanılarak taramalar yeniden boyutlandırılır ve X olarak adlandırılan 3D bir diziye kaydedilir. \npredictByPath fonksiyonu daha sonra modeli kullanarak X üzerinde tahminler yapar ve tahminleri bir dizi olarak döndürür.\nshowPredictsById fonksiyonu, modelin yaptığı tahminleri görselleştirmeyi amaçlı görünüyor. Bu fonksiyon, taramalar için ground truth verilerini yükler\"\"\"\n\n\ndef predictByPath(case_path,case):\n    files = next(os.walk(case_path))[2]\n    X = np.empty((VOLUME_SLICES, IMG_SIZE, IMG_SIZE, 2))\n\n    vol_path = os.path.join(case_path, f'BraTS19_2013_{case}_flair.nii');\n    flair=nib.load(vol_path).get_fdata()\n\n    vol_path = os.path.join(case_path, f'BraTS19_2013_{case}_t1ce.nii');\n    ce=nib.load(vol_path).get_fdata() \n\n    for j in range(VOLUME_SLICES):\n        X[j,:,:,0] = cv2.resize(flair[:,:,j+VOLUME_START_AT], (IMG_SIZE,IMG_SIZE))\n        X[j,:,:,1] = cv2.resize(ce[:,:,j+VOLUME_START_AT], (IMG_SIZE,IMG_SIZE))\n  \n    return model.predict(X/np.max(X), verbose=1)\n\ndef showPredictsById(case, start_slice = 60):\n    path = f\"/kaggle/input/brain-tumor-segmentation-brats-2019/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_2013_{case}\"\n    gt = nib.load(os.path.join(path, f'BraTS19_2013_{case}_seg.nii')).get_fdata()\n    origImage = nib.load(os.path.join(path, f'BraTS19_2013_{case}_flair.nii')).get_fdata()\n    p = predictByPath(path,case)\n\n    core = p[:,:,:,1]\n    edema= p[:,:,:,2]\n    enhancing = p[:,:,:,3]\n\n    plt.figure(figsize=(18, 50))\n    f, axarr = plt.subplots(1,6, figsize = (18, 50)) \n\n    for i in range(6):\n        axarr[i].imshow(cv2.resize(origImage[:,:,start_slice+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE)),cmap=\"gray\", interpolation='none')\n\n    axarr[0].imshow(cv2.resize(origImage[:,:,start_slice+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE)), cmap=\"gray\")\n    axarr[0].title.set_text('Original image flair')\n    curr_gt=cv2.resize(gt[:,:,start_slice+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE), interpolation = cv2.INTER_NEAREST)\n    axarr[1].imshow(curr_gt, cmap=\"Reds\", interpolation='none', alpha=0.3) # ,alpha=0.3,cmap='Reds'\n    axarr[1].title.set_text('Ground truth')\n    axarr[2].imshow(p[start_slice,:,:,1:4], cmap=\"Reds\", interpolation='none', alpha=0.3)\n    axarr[2].title.set_text('all classes')\n    axarr[3].imshow(edema[start_slice,:,:], cmap=\"OrRd\", interpolation='none', alpha=0.3)\n    axarr[3].title.set_text(f'{SEGMENT_CLASSES[1]} predicted')\n    axarr[4].imshow(core[start_slice,:,], cmap=\"OrRd\", interpolation='none', alpha=0.3)\n    axarr[4].title.set_text(f'{SEGMENT_CLASSES[2]} predicted')\n    axarr[5].imshow(enhancing[start_slice,:,], cmap=\"OrRd\", interpolation='none', alpha=0.3)\n    axarr[5].title.set_text(f'{SEGMENT_CLASSES[3]} predicted')\n    plt.show()\n\n\nshowPredictsById(case=\"5_1\")#veri deneme\n","metadata":{"execution":{"iopub.status.busy":"2023-05-09T07:55:46.591669Z","iopub.execute_input":"2023-05-09T07:55:46.592123Z","iopub.status.idle":"2023-05-09T07:55:50.391049Z","shell.execute_reply.started":"2023-05-09T07:55:46.592078Z","shell.execute_reply":"2023-05-09T07:55:50.389798Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Global Model Visualization**","metadata":{}},{"cell_type":"code","source":"#Modelin yüklenmesi\n#Modelin tahmin aşamasında kullanması için eğitim sonrası değerleri tekrar yükledik.\nmodel = keras.models.load_model('/kaggle/input/bratsparts/u-net_global_train.h5', \n                                   custom_objects={ 'accuracy' : tf.keras.metrics.MeanIoU(num_classes=4),\n                                                   \"dice_coef\": dice_coef,\n                                                   \"precision\": precision,\n                                                   \"sensitivity\":sensitivity,\n                                                   \"specificity\":specificity,\n                                                   \"dice_coef_necrotic\": dice_coef_necrotic,\n                                                   \"dice_coef_edema\": dice_coef_edema,\n                                                   \"dice_coef_enhancing\": dice_coef_enhancing\n                                                  }, compile=False)\n\n#Eğitim sonrası bize grafikleri verdi \nhistory = pd.read_csv('/kaggle/input/bratsparts/egitim_global.log', sep=',', engine='python')\n\nhist=history\n\n\n\nacc=hist['accuracy']\nval_acc=hist['val_accuracy']\n\nepoch=range(len(acc))\n\nloss=hist['loss']\nval_loss=hist['val_loss']\n\ntrain_dice=hist['dice_coef']\nval_dice=hist['val_dice_coef']\n\nf,ax=plt.subplots(1,3,figsize=(16,8))\n\nax[0].plot(epoch,acc,'b',label='Training Accuracy')\nax[0].plot(epoch,val_acc,'r',label='Validation Accuracy')\nax[0].legend()\n\nax[1].plot(epoch,loss,'b',label='Training Loss')\nax[1].plot(epoch,val_loss,'r',label='Validation Loss')\nax[1].legend()\n\nax[2].plot(epoch,train_dice,'b',label='Training dice coef')\nax[2].plot(epoch,val_dice,'r',label='Validation dice coef')\nax[2].legend()\n\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-09T07:55:50.394983Z","iopub.execute_input":"2023-05-09T07:55:50.396145Z","iopub.status.idle":"2023-05-09T07:55:51.866731Z","shell.execute_reply.started":"2023-05-09T07:55:50.396093Z","shell.execute_reply":"2023-05-09T07:55:51.865842Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the model\nmodel = keras.models.load_model('/kaggle/input/bratsparts/u-net_global_train.h5', \n                                custom_objects={ 'accuracy' : tf.keras.metrics.MeanIoU(num_classes=4),\n                                                \"dice_coef\": dice_coef,\n                                                \"precision\": precision,\n                                                \"sensitivity\":sensitivity,\n                                                \"specificity\":specificity,\n                                                \"dice_coef_necrotic\": dice_coef_necrotic,\n                                                \"dice_coef_edema\": dice_coef_edema,\n                                                \"dice_coef_enhancing\": dice_coef_enhancing\n                                               }, compile=False)\n\n# Load the training history\nhistory = pd.read_csv('/kaggle/input/bratsparts/egitim_global.log', sep=',', engine='python')\n\n# Plot the performance metrics\nf, ax = plt.subplots(2, 2, figsize=(16, 12))\n\nax[0, 0].plot(history['accuracy'], label='Training accuracy')\nax[0, 0].plot(history['val_accuracy'], label='Validation accuracy')\nax[0, 0].set_xlabel('Epoch')\nax[0, 0].set_ylabel('Accuracy')\nax[0, 0].legend()\n\nax[0, 1].plot(history['loss'], label='Training loss')\nax[0, 1].plot(history['val_loss'], label='Validation loss')\nax[0, 1].set_xlabel('Epoch')\nax[0, 1].set_ylabel('Loss')\nax[0, 1].legend()\n\nax[1, 0].plot(history['dice_coef'], label='Training dice coefficient')\nax[1, 0].plot(history['val_dice_coef'], label='Validation dice coefficient')\nax[1, 0].set_xlabel('Epoch')\nax[1, 0].set_ylabel('Dice coefficient')\nax[1, 0].legend()\n\nax[1, 1].plot(history['accuracy'], label='Training accuracy')\nax[1, 1].plot(history['val_accuracy'], label='Validation accuracy')\nax[1, 1].plot(history['dice_coef'], label='Training dice coefficient')\nax[1, 1].plot(history['val_dice_coef'], label='Validation dice coefficient')\nax[1, 1].set_xlabel('Epoch')\nax[1, 1].set_ylabel('Metric')\nax[1, 1].legend()\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-05-09T07:55:51.868361Z","iopub.execute_input":"2023-05-09T07:55:51.869011Z","iopub.status.idle":"2023-05-09T07:55:53.165866Z","shell.execute_reply.started":"2023-05-09T07:55:51.868964Z","shell.execute_reply":"2023-05-09T07:55:53.164722Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"Bu kod, eğitilmiş bir modeli kullanarak bir dizinde saklanan MRI taramaları üzerinde tahminler yapmaya yönelik görünüyor.\nÖnce nib.load fonksiyonunu kullanarak tarama verilerini yükler.\nDaha sonra, cv2.resize kullanılarak taramalar yeniden boyutlandırılır ve X olarak adlandırılan 3D bir diziye kaydedilir. \npredictByPath fonksiyonu daha sonra modeli kullanarak X üzerinde tahminler yapar ve tahminleri bir dizi olarak döndürür.\nshowPredictsById fonksiyonu, modelin yaptığı tahminleri görselleştirmeyi amaçlı görünüyor. Bu fonksiyon, taramalar için ground truth verilerini yükler\"\"\"\n\n\ndef predictByPath(case_path,case):\n    files = next(os.walk(case_path))[2]\n    X = np.empty((VOLUME_SLICES, IMG_SIZE, IMG_SIZE, 2))\n\n    vol_path = os.path.join(case_path, f'BraTS19_2013_{case}_flair.nii');\n    flair=nib.load(vol_path).get_fdata()\n\n    vol_path = os.path.join(case_path, f'BraTS19_2013_{case}_t1ce.nii');\n    ce=nib.load(vol_path).get_fdata() \n\n    for j in range(VOLUME_SLICES):\n        X[j,:,:,0] = cv2.resize(flair[:,:,j+VOLUME_START_AT], (IMG_SIZE,IMG_SIZE))\n        X[j,:,:,1] = cv2.resize(ce[:,:,j+VOLUME_START_AT], (IMG_SIZE,IMG_SIZE))\n  \n    return model.predict(X/np.max(X), verbose=1)\n\ndef showPredictsById(case, start_slice = 60):\n    path = f\"/kaggle/input/brain-tumor-segmentation-brats-2019/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_2013_{case}\"\n    gt = nib.load(os.path.join(path, f'BraTS19_2013_{case}_seg.nii')).get_fdata()\n    origImage = nib.load(os.path.join(path, f'BraTS19_2013_{case}_flair.nii')).get_fdata()\n    p = predictByPath(path,case)\n\n    core = p[:,:,:,1]\n    edema= p[:,:,:,2]\n    enhancing = p[:,:,:,3]\n\n    plt.figure(figsize=(18, 50))\n    f, axarr = plt.subplots(1,6, figsize = (18, 50)) \n\n    for i in range(6):\n        axarr[i].imshow(cv2.resize(origImage[:,:,start_slice+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE)),cmap=\"gray\", interpolation='none')\n\n    axarr[0].imshow(cv2.resize(origImage[:,:,start_slice+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE)), cmap=\"gray\")\n    axarr[0].title.set_text('Original image flair')\n    curr_gt=cv2.resize(gt[:,:,start_slice+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE), interpolation = cv2.INTER_NEAREST)\n    axarr[1].imshow(curr_gt, cmap=\"Reds\", interpolation='none', alpha=0.3) # ,alpha=0.3,cmap='Reds'\n    axarr[1].title.set_text('Ground truth')\n    axarr[2].imshow(p[start_slice,:,:,1:4], cmap=\"Reds\", interpolation='none', alpha=0.3)\n    axarr[2].title.set_text('all classes')\n    axarr[3].imshow(edema[start_slice,:,:], cmap=\"OrRd\", interpolation='none', alpha=0.3)\n    axarr[3].title.set_text(f'{SEGMENT_CLASSES[1]} predicted')\n    axarr[4].imshow(core[start_slice,:,], cmap=\"OrRd\", interpolation='none', alpha=0.3)\n    axarr[4].title.set_text(f'{SEGMENT_CLASSES[2]} predicted')\n    axarr[5].imshow(enhancing[start_slice,:,], cmap=\"OrRd\", interpolation='none', alpha=0.3)\n    axarr[5].title.set_text(f'{SEGMENT_CLASSES[3]} predicted')\n    plt.show()\n\n\nshowPredictsById(case=\"5_1\")#veri deneme\n","metadata":{"execution":{"iopub.status.busy":"2023-05-09T07:55:53.167349Z","iopub.execute_input":"2023-05-09T07:55:53.168150Z","iopub.status.idle":"2023-05-09T07:55:56.344785Z","shell.execute_reply.started":"2023-05-09T07:55:53.168111Z","shell.execute_reply":"2023-05-09T07:55:56.343856Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Compare SidebySide 3 Models**","metadata":{}},{"cell_type":"code","source":"# Load the models\nmodel1 = keras.models.load_model('/kaggle/input/bratsparts/u-net_part1.h5', \n                                custom_objects={ 'accuracy' : tf.keras.metrics.MeanIoU(num_classes=4),\n                                                \"dice_coef\": dice_coef,\n                                                \"precision\": precision,\n                                                \"sensitivity\":sensitivity,\n                                                \"specificity\":specificity,\n                                                \"dice_coef_necrotic\": dice_coef_necrotic,\n                                                \"dice_coef_edema\": dice_coef_edema,\n                                                \"dice_coef_enhancing\": dice_coef_enhancing\n                                               }, compile=False)\n\nmodel2 = keras.models.load_model('/kaggle/input/bratsparts/u-net_part2.h5', \n                                custom_objects={ 'accuracy' : tf.keras.metrics.MeanIoU(num_classes=4),\n                                                \"dice_coef\": dice_coef,\n                                                \"precision\": precision,\n                                                \"sensitivity\":sensitivity,\n                                                \"specificity\":specificity,\n                                                \"dice_coef_necrotic\": dice_coef_necrotic,\n                                                \"dice_coef_edema\": dice_coef_edema,\n                                                \"dice_coef_enhancing\": dice_coef_enhancing\n                                               }, compile=False)\n\nmodel3 = keras.models.load_model('/kaggle/input/bratsparts/u-net_part3.h5', \n                                custom_objects={ 'accuracy' : tf.keras.metrics.MeanIoU(num_classes=4),\n                                                \"dice_coef\": dice_coef,\n                                                \"precision\": precision,\n                                                \"sensitivity\":sensitivity,\n                                                \"specificity\":specificity,\n                                                \"dice_coef_necrotic\": dice_coef_necrotic,\n                                                \"dice_coef_edema\": dice_coef_edema,\n                                                \"dice_coef_enhancing\": dice_coef_enhancing\n                                               }, compile=False)\n\nmodel_global = keras.models.load_model('/kaggle/input/bratsparts/u-net_global_train.h5', \n                                custom_objects={ 'accuracy' : tf.keras.metrics.MeanIoU(num_classes=4),\n                                                \"dice_coef\": dice_coef,\n                                                \"precision\": precision,\n                                                \"sensitivity\":sensitivity,\n                                                \"specificity\":specificity,\n                                                \"dice_coef_necrotic\": dice_coef_necrotic,\n                                                \"dice_coef_edema\": dice_coef_edema,\n                                                \"dice_coef_enhancing\": dice_coef_enhancing\n                                               }, compile=False)\n\n# Load the training histories\nhistory1 = pd.read_csv('/kaggle/input/bratsparts/egitim_part1.log', sep=',', engine='python')\nhistory2 = pd.read_csv('/kaggle/input/bratsparts/egitim_part2.log', sep=',', engine='python')\nhistory3 = pd.read_csv('/kaggle/input/bratsparts/egitim_part3.log', sep=',', engine='python')\nhistory_global = pd.read_csv('/kaggle/input/bratsparts/egitim_global.log', sep=',', engine='python')\n\n\n\n# Plot the performance metrics side by side\nfig, axs = plt.subplots(4, 3, figsize=(18, 10))\n\naxs[0, 0].plot(history1['accuracy'], label='Training accuracy')\naxs[0, 0].plot(history1['val_accuracy'], label='Validation accuracy')\naxs[0, 0].set_xlabel('Epoch')\naxs[0, 0].set_ylabel('Accuracy')\naxs[0, 0].set_title('Model 1')\naxs[0, 0].legend()\n\naxs[0, 1].plot(history1['loss'], label='Training loss')\naxs[0, 1].plot(history1['val_loss'], label='Validation loss')\naxs[0, 1].set_xlabel('Epoch')\naxs[0, 1].set_ylabel('Loss')\naxs[0, 1].set_title('Model 1')\naxs[0, 1].legend()\n\naxs[0, 2].plot(history1['dice_coef'], label='Training dice coefficient')\naxs[0, 2].plot(history1['val_dice_coef'], label='Validation dice coefficient')\naxs[0, 2].set_xlabel('Epoch')\naxs[0, 2].set_ylabel('Dice coefficient')\naxs[0, 2].set_title('Model 1')\naxs[0, 2].legend()\n\naxs[1, 0].plot(history2['accuracy'], label='Training accuracy')\naxs[1, 0].plot(history2['val_accuracy'], label='Validation accuracy')\naxs[1, 0].set_xlabel('Epoch')\naxs[1, 0].set_ylabel('Accuracy')\naxs[1, 0].set_title('Model 2')\naxs[1, 0].legend()\n\naxs[1, 1].plot(history2['loss'], label='Training loss')\naxs[1, 1].plot(history2['val_loss'], label='Validation loss')\naxs[1, 1].set_xlabel('Epoch')\naxs[1, 1].set_ylabel('Loss')\naxs[1, 1].set_title('Model 2')\naxs[1, 1].legend()\n\naxs[1, 2].plot(history2['dice_coef'], label='Training dice coefficient')\naxs[1, 2].plot(history2['val_dice_coef'], label='Validation dice coefficient')\naxs[1, 2].set_xlabel('Epoch')\naxs[1, 2].set_ylabel('Dice coefficient')\naxs[1, 2].set_title('Model 2')\naxs[1, 2].legend()\n\naxs[2, 0].plot(history3['accuracy'], label='Training accuracy')\naxs[2, 0].plot(history3['val_accuracy'], label='Validation accuracy')\naxs[2, 0].set_xlabel('Epoch')\naxs[2, 0].set_ylabel('Accuracy')\naxs[2, 0].set_title('Model 3')\naxs[2, 0].legend()\n\naxs[2, 1].plot(history3['loss'], label='Training loss')\naxs[2, 1].plot(history3['val_loss'], label='Validation loss')\naxs[2, 1].set_xlabel('Epoch')\naxs[2, 1].set_ylabel('Loss')\naxs[2, 1].set_title('Model 3')\naxs[2, 1].legend()\n\naxs[2, 2].plot(history3['dice_coef'], label='Training dice coefficient')\naxs[2, 2].plot(history3['val_dice_coef'], label='Validation dice coefficient')\naxs[2, 2].set_xlabel('Epoch')\naxs[2, 2].set_ylabel('Dice coefficient')\naxs[2, 2].set_title('Model 3')\naxs[2, 2].legend()\n\naxs[3, 0].plot(history_global['accuracy'], label='Training accuracy')\naxs[3, 0].plot(history_global['val_accuracy'], label='Validation accuracy')\naxs[3, 0].set_xlabel('Epoch')\naxs[3, 0].set_ylabel('Accuracy')\naxs[3, 0].set_title('Model Global')\naxs[3, 0].legend()\n\naxs[3, 1].plot(history_global['loss'], label='Training loss')\naxs[3, 1].plot(history_global['val_loss'], label='Validation loss')\naxs[3, 1].set_xlabel('Epoch')\naxs[3, 1].set_ylabel('Loss')\naxs[3, 1].set_title('Model Global')\naxs[3, 1].legend()\n\naxs[3, 2].plot(history_global['dice_coef'], label='Training dice coefficient')\naxs[3, 2].plot(history_global['val_dice_coef'], label='Validation dice coefficient')\naxs[3, 2].set_xlabel('Epoch')\naxs[3, 2].set_ylabel('Dice coefficient')\naxs[3, 2].set_title('Model Global')\naxs[3, 2].legend()\n\n# Adjust the spacing between subplots\nplt.subplots_adjust(wspace=0.5)\nplt.subplots_adjust(hspace=0.5)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-09T07:55:56.348226Z","iopub.execute_input":"2023-05-09T07:55:56.348919Z","iopub.status.idle":"2023-05-09T07:56:00.524241Z","shell.execute_reply.started":"2023-05-09T07:55:56.348865Z","shell.execute_reply":"2023-05-09T07:56:00.523411Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot the performance metrics side by side\nfig, axs = plt.subplots(1, 1, figsize=(12, 6))\n\naxs.plot(history1['loss'], label='Model 1 Training loss')\naxs.plot(history1['val_loss'], label='Model 1 Validation loss')\n\naxs.plot(history2['loss'], label='Model 2 Training accuracy')\naxs.plot(history2['val_loss'], label='Model 2 Validation loss')\n\naxs.plot(history3['loss'], label='Model 3 Training accuracy')\naxs.plot(history3['val_loss'], label='Model 3 Validation loss')\n\naxs.plot(history3['loss'], label='Global Model Training accuracy')\naxs.plot(history3['val_loss'], label='Global Model Validation loss')\n\naxs.set_xlabel('Epoch')\naxs.set_ylabel('loss')\naxs.set_title('Comparison of Training and Validation loss for 3 Models')\naxs.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-09T07:56:00.525552Z","iopub.execute_input":"2023-05-09T07:56:00.526428Z","iopub.status.idle":"2023-05-09T07:56:00.828959Z","shell.execute_reply.started":"2023-05-09T07:56:00.526382Z","shell.execute_reply":"2023-05-09T07:56:00.827878Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot the performance metrics side by side\nfig, axs = plt.subplots(1, 1, figsize=(12, 6))\n\naxs.plot(history1['accuracy'], label='Model 1 Training accuracy')\naxs.plot(history1['val_accuracy'], label='Model 1 Validation accuracy')\n\naxs.plot(history2['accuracy'], label='Model 2 Training accuracy')\naxs.plot(history2['val_accuracy'], label='Model 2 Validation accuracy')\n\naxs.plot(history3['accuracy'], label='Model 3 Training accuracy')\naxs.plot(history3['val_accuracy'], label='Model 3 Validation accuracy')\n\naxs.plot(history3['accuracy'], label='Global Model Training accuracy')\naxs.plot(history3['val_accuracy'], label='Global Model Validation accuracy')\n\naxs.set_xlabel('Epoch')\naxs.set_ylabel('Accuracy')\naxs.set_title('Comparison of Training and Validation Accuracy for 3 Models')\naxs.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-09T07:56:00.830482Z","iopub.execute_input":"2023-05-09T07:56:00.831637Z","iopub.status.idle":"2023-05-09T07:56:01.218334Z","shell.execute_reply.started":"2023-05-09T07:56:00.831594Z","shell.execute_reply":"2023-05-09T07:56:01.217152Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot the performance metrics side by side\nfig, axs = plt.subplots(1, 1, figsize=(12, 6))\n\naxs.plot(history1['dice_coef'], label='Model 1 Training Dice coefficient')\naxs.plot(history1['val_dice_coef'], label='Model 1 Validation Dice coefficient')\n\naxs.plot(history2['dice_coef'], label='Model 2 Training Dice coefficient')\naxs.plot(history2['val_dice_coef'], label='Model 2 Validation Dice coefficient')\n\naxs.plot(history3['dice_coef'], label='Model 3 Training Dice coefficient')\naxs.plot(history3['val_dice_coef'], label='Model 3 Validation Dice coefficient')\n\naxs.plot(history3['dice_coef'], label='Global Model Training Dice coefficient')\naxs.plot(history3['val_dice_coef'], label='Global Model Validation Dice coefficient')\n\naxs.set_xlabel('Epoch')\naxs.set_ylabel('Dice coefficient')\naxs.set_title('Comparison of Training and Validation Dice coefficient for 3 Models')\naxs.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-09T07:56:01.219930Z","iopub.execute_input":"2023-05-09T07:56:01.220296Z","iopub.status.idle":"2023-05-09T07:56:01.610238Z","shell.execute_reply.started":"2023-05-09T07:56:01.220262Z","shell.execute_reply":"2023-05-09T07:56:01.608857Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Extract final epoch metrics\ndef extract_final_metrics(history):\n    final_epoch = history.iloc[-1]\n    metrics = {\n        \"Training Accuracy\": final_epoch[\"accuracy\"],\n        \"Validation Accuracy\": final_epoch[\"val_accuracy\"],\n        \"Training Loss\": final_epoch[\"loss\"],\n        \"Validation Loss\": final_epoch[\"val_loss\"],\n        \"Training Dice Coefficient\": final_epoch[\"dice_coef\"],\n        \"Validation Dice Coefficient\": final_epoch[\"val_dice_coef\"],\n    }\n    return metrics\n\n# Create a DataFrame to store metrics\nmetrics_table = pd.DataFrame(columns=[\"Model\", \"Training Accuracy\", \"Validation Accuracy\",\n                                      \"Training Loss\", \"Validation Loss\",\n                                      \"Training Dice Coefficient\", \"Validation Dice Coefficient\"])\n\n# Extract metrics from each model's history\nmetrics_model1 = extract_final_metrics(history1)\nmetrics_model2 = extract_final_metrics(history2)\nmetrics_model3 = extract_final_metrics(history3)\nmetrics_global = extract_final_metrics(history_global)\n\n# Add metrics to the DataFrame\nmetrics_table = metrics_table.append({**{\"Model\": \"Model 1\"}, **metrics_model1}, ignore_index=True)\nmetrics_table = metrics_table.append({**{\"Model\": \"Model 2\"}, **metrics_model2}, ignore_index=True)\nmetrics_table = metrics_table.append({**{\"Model\": \"Model 3\"}, **metrics_model3}, ignore_index=True)\nmetrics_table = metrics_table.append({**{\"Model\": \"Model Global\"}, **metrics_global}, ignore_index=True)\n\n# Display the table\nprint(metrics_table)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-09T07:56:01.611360Z","iopub.execute_input":"2023-05-09T07:56:01.611692Z","iopub.status.idle":"2023-05-09T07:56:01.643780Z","shell.execute_reply.started":"2023-05-09T07:56:01.611662Z","shell.execute_reply":"2023-05-09T07:56:01.642641Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot the line graph\ndef plot_metrics(metrics_df, metric_name):\n    plt.figure()\n    for index, row in metrics_df.iterrows():\n        plt.plot(history_dict[row[\"Model\"]].index, history_dict[row[\"Model\"]][metric_name], label=row[\"Model\"])\n    plt.xlabel('Epoch')\n    plt.ylabel(metric_name)\n    plt.title(f'{metric_name} per Epoch')\n    plt.legend()\n    plt.show()\n\n# Combine all histories into one dictionary\nhistory_dict = {\n    \"Model 1\": history1,\n    \"Model 2\": history2,\n    \"Model 3\": history3,\n    \"Model Global\": history_global\n}\n\n# Plot the line graphs for the desired metrics\nplot_metrics(metrics_table, \"accuracy\")\nplot_metrics(metrics_table, \"val_accuracy\")\nplot_metrics(metrics_table, \"loss\")\nplot_metrics(metrics_table, \"val_loss\")\nplot_metrics(metrics_table, \"dice_coef\")\nplot_metrics(metrics_table, \"val_dice_coef\")","metadata":{"execution":{"iopub.status.busy":"2023-05-09T07:57:16.608359Z","iopub.execute_input":"2023-05-09T07:57:16.608787Z","iopub.status.idle":"2023-05-09T07:57:18.418985Z","shell.execute_reply.started":"2023-05-09T07:57:16.608749Z","shell.execute_reply":"2023-05-09T07:57:18.417888Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot the bar graph\ndef plot_metrics(metrics_df, metric_name):\n    plt.figure()\n    models = metrics_df[\"Model\"].values\n    metric_values = metrics_df[metric_name].values\n    plt.bar(models, metric_values)\n    plt.xlabel('Model')\n    plt.ylabel(metric_name)\n    plt.title(f'{metric_name} for Each Model')\n    plt.show()\n\n# Plot the bar graphs for the desired metrics\nplot_metrics(metrics_table, \"Training Accuracy\")\nplot_metrics(metrics_table, \"Validation Accuracy\")\nplot_metrics(metrics_table, \"Training Loss\")\nplot_metrics(metrics_table, \"Validation Loss\")\nplot_metrics(metrics_table, \"Training Dice Coefficient\")\nplot_metrics(metrics_table, \"Validation Dice Coefficient\")\n","metadata":{"execution":{"iopub.status.busy":"2023-05-09T07:58:31.701281Z","iopub.execute_input":"2023-05-09T07:58:31.702044Z","iopub.status.idle":"2023-05-09T07:58:32.843265Z","shell.execute_reply.started":"2023-05-09T07:58:31.702004Z","shell.execute_reply":"2023-05-09T07:58:32.842122Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plot the line graph\ndef plot_metrics(metrics_df, metric_name):\n    plt.figure()\n    models = metrics_df[\"Model\"].values\n    metric_values = metrics_df[metric_name].values\n    plt.plot(models, metric_values, marker='o')\n    plt.xlabel('Model')\n    plt.ylabel(metric_name)\n    plt.title(f'{metric_name} for Each Model')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-09T08:00:09.984427Z","iopub.execute_input":"2023-05-09T08:00:09.984854Z","iopub.status.idle":"2023-05-09T08:00:09.991660Z","shell.execute_reply.started":"2023-05-09T08:00:09.984806Z","shell.execute_reply":"2023-05-09T08:00:09.990359Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot the line graphs for the desired metrics\nplot_metrics(metrics_table, \"Training Accuracy\")","metadata":{"execution":{"iopub.status.busy":"2023-05-09T08:00:19.383032Z","iopub.execute_input":"2023-05-09T08:00:19.383442Z","iopub.status.idle":"2023-05-09T08:00:19.611131Z","shell.execute_reply.started":"2023-05-09T08:00:19.383407Z","shell.execute_reply":"2023-05-09T08:00:19.610026Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot the line graphs for the desired metrics\nplot_metrics(metrics_table, \"Validation Accuracy\")","metadata":{"execution":{"iopub.status.busy":"2023-05-09T08:00:29.619229Z","iopub.execute_input":"2023-05-09T08:00:29.620186Z","iopub.status.idle":"2023-05-09T08:00:29.857003Z","shell.execute_reply.started":"2023-05-09T08:00:29.620141Z","shell.execute_reply":"2023-05-09T08:00:29.855640Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot the line graphs for the desired metrics\nplot_metrics(metrics_table, \"Training Loss\")","metadata":{"execution":{"iopub.status.busy":"2023-05-09T08:00:43.339003Z","iopub.execute_input":"2023-05-09T08:00:43.339924Z","iopub.status.idle":"2023-05-09T08:00:43.574044Z","shell.execute_reply.started":"2023-05-09T08:00:43.339883Z","shell.execute_reply":"2023-05-09T08:00:43.572885Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot the line graphs for the desired metrics\nplot_metrics(metrics_table, \"Validation Loss\")","metadata":{"execution":{"iopub.status.busy":"2023-05-09T08:00:56.179391Z","iopub.execute_input":"2023-05-09T08:00:56.180744Z","iopub.status.idle":"2023-05-09T08:00:56.414662Z","shell.execute_reply.started":"2023-05-09T08:00:56.180682Z","shell.execute_reply":"2023-05-09T08:00:56.413609Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot the line graphs for the desired metrics\nplot_metrics(metrics_table, \"Training Dice Coefficient\")","metadata":{"execution":{"iopub.status.busy":"2023-05-09T08:01:06.833643Z","iopub.execute_input":"2023-05-09T08:01:06.834649Z","iopub.status.idle":"2023-05-09T08:01:07.049846Z","shell.execute_reply.started":"2023-05-09T08:01:06.834609Z","shell.execute_reply":"2023-05-09T08:01:07.048789Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot the line graphs for the desired metrics\nplot_metrics(metrics_table, \"Validation Dice Coefficient\")","metadata":{"execution":{"iopub.status.busy":"2023-05-09T08:01:15.886353Z","iopub.execute_input":"2023-05-09T08:01:15.886877Z","iopub.status.idle":"2023-05-09T08:01:16.108681Z","shell.execute_reply.started":"2023-05-09T08:01:15.886808Z","shell.execute_reply":"2023-05-09T08:01:16.107652Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Retrain with Global Model that used Brats 2019 Dataset**\n\nBased on the updated table provided, we can summarize the performance of the 4 models as follows:\n\n1. **Model 1**: This model has the highest training accuracy (99.31%) and the second-highest validation accuracy (99.17%). It has a relatively low training loss (0.0195) and validation loss (0.0279). The training dice coefficient is 0.4856, and the validation dice coefficient is 0.4395.\n\n2. **Model 2**: Model 2 shows a slightly lower training accuracy (99.15%) and validation accuracy (98.98%) compared to Model 1. The training loss is 0.0239, and the validation loss is 0.0327. This model has the highest training dice coefficient (0.5763) and validation dice coefficient (0.5370) among all models.\n\n3. **Model 3**: This model has the lowest training accuracy (98.44%) and validation accuracy (98.17%) among the models. It also has the highest training loss (0.0442) and validation loss (0.0545). The training dice coefficient is 0.3888, and the validation dice coefficient is 0.3551, which are the lowest among all models.\n\n4. **Model Global**: The global model, created by aggregating the other models using federated learning, has a training accuracy of 99.23% and the highest validation accuracy (99.14%). It has a training loss of 0.0229 and a validation loss of 0.0293, which is higher than Model 1 but lower than the other models. The training dice coefficient is 0.5533, and the validation dice coefficient is 0.5399, which are both higher than Model 1 and Model 3 but slightly lower than Model 2.\n\nIn summary, the global model has the best overall performance in terms of validation accuracy. Model 2 has the highest dice coefficients, which could be useful depending on the specific requirements of your application. Model 1 has the lowest validation loss, so it might perform better in terms of generalization.\n\n\nGiven that Model 1, Model 2, and Model 3 are trained on the BraTS 2020 dataset, and the global model is retrained using the BraTS 2019 dataset, we can draw the following conclusions:\n\n1. **Model Global**: The fact that the global model has the highest validation accuracy (99.14%) despite being retrained on the BraTS 2019 dataset indicates that the model has generalized well across different datasets. This suggests that the federated learning approach is effective in combining the knowledge of the three individual models, resulting in a better overall performance.\n\n2. **Individual Models**: Since the three individual models (Model 1, Model 2, and Model 3) are trained on the BraTS 2020 dataset, their performance metrics are specific to that dataset. Comparing the individual models, Model 2 has the highest training and validation dice coefficients, making it the best-performing model in terms of segmentation quality. Model 1 has the lowest validation loss, so it might perform better in terms of generalization.\n\nIt is essential to note that the global model's performance indicates that it could be a better choice for applications that require a model that generalizes well across different datasets. However, if the primary concern is segmentation quality, Model 2 seems to be the best choice among the individual models.","metadata":{}},{"cell_type":"markdown","source":"**Old One With Global Part 1**\n\nBased on the table provided, we can summarize the performance of the 4 models as follows:\n\n1. **Model 1**: This model has the highest training accuracy (99.31%) and second-highest validation accuracy (99.17%). It has a relatively low training loss (0.0195) and validation loss (0.0279). The training dice coefficient is 0.4856, and the validation dice coefficient is 0.4395.\n\n2. **Model 2**: Model 2 shows a slightly lower training accuracy (99.15%) and validation accuracy (98.98%) compared to Model 1. The training loss is 0.0239, and the validation loss is 0.0327. This model has the highest training dice coefficient (0.5763) and validation dice coefficient (0.5370) among all models.\n\n3. **Model 3**: This model has the lowest training accuracy (98.44%) and validation accuracy (98.17%) among the models. It also has the highest training loss (0.0442) and validation loss (0.0545). The training dice coefficient is 0.3888, and the validation dice coefficient is 0.3551, which are the lowest among all models.\n\n4. **Model Global**: The global model, created by aggregating the other models using federated learning, has a training accuracy of 99.23% and the highest validation accuracy (99.32%). It has a training loss of 0.0211 and the lowest validation loss (0.0209). The training dice coefficient is 0.4616, and the validation dice coefficient is 0.4713.\n\nIn summary, the global model has the best overall performance, with the highest validation accuracy and the lowest validation loss. Model 2 has the highest dice coefficients, which could be useful depending on the specific requirements of your application.","metadata":{}},{"cell_type":"code","source":"print(tf.__version__)","metadata":{"execution":{"iopub.status.busy":"2023-05-09T07:56:01.645410Z","iopub.execute_input":"2023-05-09T07:56:01.645885Z","iopub.status.idle":"2023-05-09T07:56:01.652633Z","shell.execute_reply.started":"2023-05-09T07:56:01.645838Z","shell.execute_reply":"2023-05-09T07:56:01.651316Z"},"trusted":true},"outputs":[],"execution_count":null}]}